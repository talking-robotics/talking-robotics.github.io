---
layout: talk
type: "Talk"
date: 2021-10-01
name: "Daniel Brown"
teaser: "Leveraging Human Input for Robust Robot Learning"
link: "/all_talks/danielbrown"
---
## Speaker

Daniel is a postdoc at [UC Berkeley](https://www.berkeley.edu/), advised by Anca Dragan and Ken Goldberg. His research interests include robot learning, reward inference, AI safety, and multi-agent systems. He received his Ph.D. in computer science from the University of Texas at Austin, where he worked with Scott Niekum on safe imitation learning. Prior to starting his PhD, Daniel worked for the [Air Force Research Lab](https://www.afrl.af.mil/)'s Information Directorate where he studied bio-inspired swarms and multi-agent planning.

Speaker Links: [Google Scholar](https://scholar.google.com/citations?user=A3wg18wAAAAJ&hl=en) - [Website](https://people.eecs.berkeley.edu/~dsbrown/)- [Linkedin](https://www.linkedin.com/in/daniel-brown-0ba93b21/) - [Twitter](https://twitter.com/daniel_s_brown?lang=en) - [Github](https://github.com/dsbrown1331)

<iframe width="560" height="315" src="https://www.youtube.com/embed/okKbRMawxPI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---

## Abstract
In this talk he will discuss recent work that seeks to develop robots that use human input to enable robust learning. In particular, he will focus on autonomous systems that seek to learn reward functions and policies from human demonstrations and preferences. One problem that arises when learning from human input is that there is often a large amount of uncertainty over the human’s true intent and the corresponding desired robot behavior. To address this problem, he will discuss research on how to enable robots to maintain efficient and accurate representations of their uncertainty, how robots can use these representations of uncertainty to generate risk-averse solutions, and how a robot can actively query for additional human feedback to reduce its uncertainty over the human’s intent and improve the robustness of its learned policy.

---

#### Papers covered during the talk
* **Brown, D.S.**, Coleman, R., Srinivasan, R.R., & Niekum, S. (2020). Safe Imitation Learning via Fast Bayesian Reward Inference from Preferences. ICML. [pdf](https://arxiv.org/pdf/2002.09089.pdf)

* Javed, Z., **Brown, D.S.**, Sharma, S., Zhu, J., Balakrishna, A., Petrik, M., Dragan, A., & Goldberg, K. (2021). Policy Gradient Bayesian Robust Optimization for Imitation Learning. ArXiv, abs/2106.06499. [pdf](https://arxiv.org/pdf/2106.06499.pdf)

* Hoque, R., Balakrishna, A., Putterman, C., Luo, M., **Brown, D.S.**, Seita, D., Thananjeyan, B., Novoseller, E.R., & Goldberg, K. (2021). LazyDAgger: Reducing Context Switching in Interactive Imitation Learning. ArXiv, abs/2104.00053. [pdf](https://arxiv.org/pdf/2104.00053.pdf)

* Zurek, M., Bobu, A., **Brown, D.S.**, & Dragan, A. (2021). Situational Confidence Assistance for Lifelong Shared Autonomy. ArXiv, abs/2104.06556. [pdf](https://arxiv.org/abs/2104.06556)
